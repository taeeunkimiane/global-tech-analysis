# advanced_analytics.py
import pandas as pd
import numpy as np
import networkx as nx
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
from sklearn.decomposition import LatentDirichletAllocation
from sklearn.metrics.pairwise import cosine_similarity
import plotly.graph_objects as go
import plotly.express as px
from plotly.subplots import make_subplots
from textblob import TextBlob
from wordcloud import WordCloud
import matplotlib.pyplot as plt
from datetime import datetime, timedelta
import json
from typing import List, Dict, Tuple
import logging
from collections import Counter, defaultdict
import re

logger = logging.getLogger(__name__)

class TechTrendAnalyzer:
    def __init__(self, articles: List[Dict]):
        self.articles = articles
        self.df = pd.DataFrame(articles)
        self.df['date'] = pd.to_datetime(self.df['date'])
        
        # ì „ì²˜ë¦¬
        self._preprocess_data()
        
    def _preprocess_data(self):
        """ë°ì´í„° ì „ì²˜ë¦¬"""
        # í…ìŠ¤íŠ¸ ê²°í•©
        self.df['full_text'] = self.df['title'] + ' ' + self.df['content']
        
        # í…ìŠ¤íŠ¸ ì •ë¦¬
        self.df['clean_text'] = self.df['full_text'].apply(self._clean_text)
        
        # ë‚ ì§œ ê´€ë ¨ í”¼ì²˜ ì¶”ê°€
        self.df['year'] = self.df['date'].dt.year
        self.df['month'] = self.df['date'].dt.month
        self.df['day_of_week'] = self.df['date'].dt.day_name()
        
    def _clean_text(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ ì •ë¦¬"""
        # HTML íƒœê·¸ ì œê±°
        text = re.sub(r'<[^>]+>', '', text)
        # íŠ¹ìˆ˜ë¬¸ì ì œê±° (ë‹¨, í•œê¸€, ì˜ë¬¸, ìˆ«ìëŠ” ìœ ì§€)
        text = re.sub(r'[^\w\sê°€-í£]', ' ', text)
        # ì—¬ëŸ¬ ê³µë°±ì„ í•˜ë‚˜ë¡œ
        text = re.sub(r'\s+', ' ', text)
        return text.strip().lower()

    def get_trending_topics(self, n_topics: int = 5) -> Dict:
        """í† í”½ ëª¨ë¸ë§ì„ í†µí•œ íŠ¸ë Œë”© ì£¼ì œ ë¶„ì„"""
        
        # TF-IDF ë²¡í„°í™”
        vectorizer = TfidfVectorizer(
            max_features=1000,
            stop_words='english',
            ngram_range=(1, 2),
            min_df=2,
            max_df=0.8
        )
        
        tfidf_matrix = vectorizer.fit_transform(self.df['clean_text'])
        
        # LDA í† í”½ ëª¨ë¸ë§
        lda = LatentDirichletAllocation(
            n_components=n_topics,
            random_state=42,
            max_iter=10
        )
        
        lda.fit(tfidf_matrix)
        
        # í† í”½ë³„ ì£¼ìš” ë‹¨ì–´ ì¶”ì¶œ
        feature_names = vectorizer.get_feature_names_out()
        topics = {}
        
        for topic_idx, topic in enumerate(lda.components_):
            top_words_idx = topic.argsort()[-10:][::-1]
            top_words = [feature_names[i] for i in top_words_idx]
            topics[f"Topic_{topic_idx + 1}"] = {
                'words': top_words,
                'weight': topic[top_words_idx].tolist()
            }
        
        # ë¬¸ì„œ-í† í”½ ë¶„í¬
        doc_topic_dist = lda.transform(tfidf_matrix)
        
        return {
            'topics': topics,
            'doc_topic_distribution': doc_topic_dist,
            'vectorizer': vectorizer,
            'lda_model': lda
        }

    def analyze_country_tech_focus(self) -> Dict:
        """êµ­ê°€ë³„ ê¸°ìˆ  ì§‘ì¤‘ë„ ë¶„ì„"""
        country_tech = defaultdict(lambda: defaultdict(int))
        
        for _, row in self.df.iterrows():
            country_tech[row['country']][row['category']] += 1
        
        # ì •ê·œí™” (ê° êµ­ê°€ì˜ ì´ ê¸°ì‚¬ ìˆ˜ë¡œ ë‚˜ëˆ„ê¸°)
        country_tech_normalized = {}
        for country, tech_counts in country_tech.items():
            total_articles = sum(tech_counts.values())
            country_tech_normalized[country] = {
                tech: count / total_articles
                for tech, count in tech_counts.items()
            }
        
        return country_tech_normalized

    def detect_emerging_trends(self, window_days: int = 7) -> Dict:
        """ì‹ í¥ íŠ¸ë Œë“œ ê°ì§€"""
        recent_date = self.df['date'].max()
        cutoff_date = recent_date - timedelta(days=window_days)
        
        recent_articles = self.df[self.df['date'] >= cutoff_date]
        older_articles = self.df[self.df['date'] < cutoff_date]
        
        # ìµœê·¼ ê¸°ê°„ê³¼ ì´ì „ ê¸°ê°„ì˜ í‚¤ì›Œë“œ ë¹ˆë„ ë¹„êµ
        recent_keywords = self._extract_keywords(recent_articles['clean_text'])
        older_keywords = self._extract_keywords(older_articles['clean_text'])
        
        # íŠ¸ë Œë“œ ìŠ¤ì½”ì–´ ê³„ì‚° (ìµœê·¼ ë¹ˆë„ / ì´ì „ ë¹ˆë„)
        trending_keywords = {}
        for keyword, recent_freq in recent_keywords.items():
            older_freq = older_keywords.get(keyword, 1)  # 0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ë°©ì§€
            trend_score = recent_freq / older_freq
            
            if recent_freq >= 3 and trend_score > 2:  # ìµœì†Œ ë¹ˆë„ì™€ íŠ¸ë Œë“œ ìŠ¤ì½”ì–´ ê¸°ì¤€
                trending_keywords[keyword] = {
                    'recent_frequency': recent_freq,
                    'older_frequency': older_freq,
                    'trend_score': trend_score
                }
        
        # íŠ¸ë Œë“œ ìŠ¤ì½”ì–´ ìˆœìœ¼ë¡œ ì •ë ¬
        trending_keywords = dict(
            sorted(trending_keywords.items(), 
                   key=lambda x: x[1]['trend_score'], 
                   reverse=True)
        )
        
        return trending_keywords

    def _extract_keywords(self, texts: pd.Series, n_keywords: int = 50) -> Dict[str, int]:
        """í…ìŠ¤íŠ¸ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        vectorizer = TfidfVectorizer(
            max_features=n_keywords,
            stop_words='english',
            ngram_range=(1, 2),
            min_df=2
        )
        
        tfidf_matrix = vectorizer.fit_transform(texts)
        feature_names = vectorizer.get_feature_names_out()
        
        # ê° í‚¤ì›Œë“œì˜ ì´ TF-IDF ì ìˆ˜ ê³„ì‚°
        keyword_scores = {}
        for i, keyword in enumerate(feature_names):
            keyword_scores[keyword] = tfidf_matrix[:, i].sum()
        
        return dict(Counter(keyword_scores).most_common(n_keywords))

    def sentiment_trend_analysis(self) -> Dict:
        """ê°ì • íŠ¸ë Œë“œ ë¶„ì„"""
        # ë‚ ì§œë³„ í‰ê·  ê°ì • ê³„ì‚°
        daily_sentiment = self.df.groupby('date')['sentiment'].agg(['mean', 'count']).reset_index()
        
        # êµ­ê°€ë³„ ê°ì • ë¶„ì„
        country_sentiment = self.df.groupby('country')['sentiment'].agg(['mean', 'std', 'count']).reset_index()
        
        # ì¹´í…Œê³ ë¦¬ë³„ ê°ì • ë¶„ì„
        category_sentiment = self.df.groupby('category')['sentiment'].agg(['mean', 'std', 'count']).reset_index()
        
        return {
            'daily_trend': daily_sentiment,
            'country_sentiment': country_sentiment,
            'category_sentiment': category_sentiment
        }

    def create_technology_network(self) -> nx.Graph:
        """ê¸°ìˆ  ê°„ ì—°ê´€ì„± ë„¤íŠ¸ì›Œí¬ ìƒì„±"""
        # ê¸°ì‚¬ë³„ë¡œ ì–¸ê¸‰ëœ ê¸°ìˆ ë“¤ì„ ì¶”ì¶œ
        tech_cooccurrence = defaultdict(lambda: defaultdict(int))
        
        # ê¸°ìˆ  í‚¤ì›Œë“œ ì •ì˜ (í™•ì¥ëœ ë²„ì „)
        tech_keywords = {
            'ìŠ¤í•€íŠ¸ë¡œë‹‰ìŠ¤': ['spintronics', 'ìŠ¤í•€íŠ¸ë¡œë‹‰ìŠ¤', 'spin electronics'],
            'ì–‘ìì»´í“¨íŒ…': ['quantum computing', 'ì–‘ìì»´í“¨íŒ…', 'quantum computer'],
            'AI': ['artificial intelligence', 'ì¸ê³µì§€ëŠ¥', 'machine learning', 'deep learning'],
            'ììœ¨ì£¼í–‰': ['autonomous vehicle', 'ììœ¨ì£¼í–‰', 'self-driving'],
            'ë¸”ë¡ì²´ì¸': ['blockchain', 'ë¸”ë¡ì²´ì¸', 'cryptocurrency'],
            '5G': ['5G', '5g network', '5ì„¸ëŒ€'],
            'IoT': ['IoT', 'internet of things', 'ì‚¬ë¬¼ì¸í„°ë„·'],
            'AR/VR': ['augmented reality', 'virtual reality', 'AR', 'VR', 'ì¦ê°•í˜„ì‹¤', 'ê°€ìƒí˜„ì‹¤']
        }
        
        # ê° ê¸°ì‚¬ì—ì„œ ì–¸ê¸‰ëœ ê¸°ìˆ ë“¤ ì°¾ê¸°
        for _, row in self.df.iterrows():
            text = row['clean_text']
            mentioned_techs = []
            
            for tech_name, keywords in tech_keywords.items():
                if any(keyword.lower() in text for keyword in keywords):
                    mentioned_techs.append(tech_name)
            
            # ë™ì‹œ ì¶œí˜„ ê¸°ë¡
            for i, tech1 in enumerate(mentioned_techs):
                for tech2 in mentioned_techs[i+1:]:
                    tech_cooccurrence[tech1][tech2] += 1
                    tech_cooccurrence[tech2][tech1] += 1
        
        # ë„¤íŠ¸ì›Œí¬ ê·¸ë˜í”„ ìƒì„±
        G = nx.Graph()
        
        for tech1, connections in tech_cooccurrence.items():
            for tech2, weight in connections.items():
                if weight >= 2:  # ìµœì†Œ 2ë²ˆ ì´ìƒ ë™ì‹œ ì¶œí˜„
                    G.add_edge(tech1, tech2, weight=weight)
        
        return G

    def generate_insights_report(self) -> str:
        """ì¸ì‚¬ì´íŠ¸ ë³´ê³ ì„œ ìƒì„±"""
        report = []
        
        # ê¸°ë³¸ í†µê³„
        total_articles = len(self.df)
        date_range = f"{self.df['date'].min().strftime('%Y-%m-%d')} ~ {self.df['date'].max().strftime('%Y-%m-%d')}"
        
        report.append(f"ğŸ“Š **ë¶„ì„ ê¸°ê°„**: {date_range}")
        report.append(f"ğŸ“° **ì´ ê¸°ì‚¬ ìˆ˜**: {total_articles:,}ê°œ")
        report.append("")
        
        # êµ­ê°€ë³„ í™œë™ë„
        country_counts = self.df['country'].value_counts()
        report.append("ğŸŒ **êµ­ê°€ë³„ ê¸°ìˆ  ì´ìŠˆ í™œë™ë„**")
        for country, count in country_counts.head(5).items():
            percentage = (count / total_articles) * 100
            report.append(f"- {country}: {count}ê°œ ({percentage:.1f}%)")
        report.append("")
        
        # ê¸°ìˆ  ë¶„ì•¼ë³„ ê´€ì‹¬ë„
        category_counts = self.df['category'].value_counts()
        report.append("ğŸ”¬ **ê¸°ìˆ  ë¶„ì•¼ë³„ ê´€ì‹¬ë„**")
        for category, count in category_counts.head(5).items():
            percentage = (count / total_articles) * 100
            report.append(f"- {category}: {count}ê°œ ({percentage:.1f}%)")
        report.append("")
        
        # ê°ì • ë¶„ì„ ê²°ê³¼
        avg_sentiment = self.df['sentiment'].mean()
        sentiment_label = "ê¸ì •ì " if avg_sentiment > 0.1 else "ë¶€ì •ì " if avg_sentiment < -0.1 else "ì¤‘ë¦½ì "
        report.append("ğŸ­ **ì „ë°˜ì  ì—¬ë¡ **")
        report.append(f"- í‰ê·  ê°ì • ì ìˆ˜: {avg_sentiment:.3f} ({sentiment_label})")
        report.append("")
        
        # ì‹ í¥ íŠ¸ë Œë“œ
        trending = self.detect_emerging_trends()
        if trending:
            report.append("ğŸ“ˆ **ì‹ í¥ íŠ¸ë Œë“œ í‚¤ì›Œë“œ** (ìµœê·¼ 7ì¼)")
            for keyword, data in list(trending.items())[:5]:
                report.append(f"- {keyword}: íŠ¸ë Œë“œ ì ìˆ˜ {data['trend_score']:.1f}x")
            report.append("")
        
        # ê¸°ìˆ  ì§‘ì¤‘ë„ê°€ ë†’ì€ êµ­ê°€
        country_focus = self.analyze_country_tech_focus()
        report.append("ğŸ¯ **êµ­ê°€ë³„ ê¸°ìˆ  íŠ¹í™” ë¶„ì•¼**")
        for country, tech_dist in list(country_focus.items())[:5]:
            if tech_dist:
                top_tech = max(tech_dist, key=tech_dist.get)
                focus_rate = tech_dist[top_tech]
                report.append(f"- {country}: {top_tech} ({focus_rate:.1%} ì§‘ì¤‘)")
        
        return "\n".join(report)

class AdvancedVisualizer:
    def __init__(self, analyzer: TechTrendAnalyzer):
        self.analyzer = analyzer
        self.df = analyzer.df
    
    def create_network_visualization(self) -> go.Figure:
        """ê¸°ìˆ  ë„¤íŠ¸ì›Œí¬ ì‹œê°í™”"""
        G = self.analyzer.create_technology_network()
        
        if len(G.nodes()) == 0:
            # ë¹ˆ ê·¸ë˜í”„ì¸ ê²½ìš°
            fig = go.Figure()
            fig.add_annotation(
                text="ì¶©ë¶„í•œ ê¸°ìˆ  ì—°ê´€ì„± ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.",
                xref="paper", yref="paper",
                x=0.5, y=0.5, xanchor='center', yanchor='middle',
                showarrow=False, font=dict(size=16)
            )
            return fig
        
        # ë…¸ë“œ ìœ„ì¹˜ ê³„ì‚°
        pos = nx.spring_layout(G, k=3, iterations=50)
        
        # ì—£ì§€ ê·¸ë¦¬ê¸°
        edge_x = []
        edge_y = []
        edge_info = []
        
        for edge in G.edges():
            x0, y0 = pos[edge[0]]
            x1, y1 = pos[edge[1]]
            edge_x.extend([x0, x1, None])
            edge_y.extend([y0, y1, None])
            weight = G[edge[0]][edge[1]]['weight']
            edge_info.append(f"{edge[0]} â†” {edge[1]}: {weight}ê°œ ê¸°ì‚¬ì—ì„œ í•¨ê»˜ ì–¸ê¸‰")
        
        edge_trace = go.Scatter(
            x=edge_x, y=edge_y,
            line=dict(width=2, color='#888'),
            hoverinfo='none',
            mode='lines'
        )
        
        # ë…¸ë“œ ê·¸ë¦¬ê¸°
        node_x = []
        node_y = []
        node_text = []
        node_info = []
        node_size = []
        
        for node in G.nodes():
            x, y = pos[node]
            node_x.append(x)
            node_y.append(y)
            node_text.append(node)
            
            # ë…¸ë“œ í¬ê¸°ëŠ” ì—°ê²°ëœ ì—£ì§€ ìˆ˜ì— ë¹„ë¡€
            connections = len(list(G.neighbors(node)))
            node_size.append(20 + connections * 10)
            node_info.append(f"{node}<br>ì—°ê²°ëœ ê¸°ìˆ : {connections}ê°œ")
        
        node_trace = go.Scatter(
            x=node_x, y=node_y,
            mode='markers+text',
            hoverinfo='text',
            text=node_text,
            textposition="middle center",
            hovertext=node_info,
            marker=dict(
                size=node_size,
                color='lightblue',
                line=dict(width=2, color='darkblue')
            )
        )
        
        fig = go.Figure(data=[edge_trace, node_trace],
                       layout=go.Layout(
                           title='ê¸°ìˆ  ê°„ ì—°ê´€ì„± ë„¤íŠ¸ì›Œí¬',
                           titlefont_size=16,
                           showlegend=False,
                           hovermode='closest',
                           margin=dict(b=20,l=5,r=5,t=40),
                           annotations=[ dict(
                               text="ë…¸ë“œ í¬ê¸°ëŠ” ë‹¤ë¥¸ ê¸°ìˆ ê³¼ì˜ ì—°ê´€ì„± ì •ë„ë¥¼ ë‚˜íƒ€ëƒ„",
                               showarrow=False,
                               xref="paper", yref="paper",
                               x=0.005, y=-0.002 ) ],
                           xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),
                           yaxis=dict(showgrid=False, zeroline=False, showticklabels=False)
                       ))
        
        return fig
    
    def create_sentiment_timeline(self) -> go.Figure:
        """ê°ì • íŠ¸ë Œë“œ íƒ€ì„ë¼ì¸"""
        sentiment_data = self.analyzer.sentiment_trend_analysis()
        daily_sentiment = sentiment_data['daily_trend']
        
        fig = go.Figure()
        
        # ì „ì²´ ê°ì • íŠ¸ë Œë“œ
        fig.add_trace(go.Scatter(
            x=daily_sentiment['date'],
            y=daily_sentiment['mean'],
            mode='lines+markers',
            name='ì¼í‰ê·  ê°ì •',
            line=dict(color='blue', width=3),
            hovertemplate='%{x}<br>ê°ì • ì ìˆ˜: %{y:.3f}<br>ê¸°ì‚¬ ìˆ˜: %{customdata}<extra></extra>',
            customdata=daily_sentiment['count']
        ))
        
        # 0ì„  ì¶”ê°€
        fig.add_hline(y=0, line_dash="dash", line_color="gray", 
                     annotation_text="ì¤‘ë¦½ì„ ", annotation_position="bottom right")
        
        # ê¸ì •/ë¶€ì • ì˜ì—­ ìƒ‰ì¹ 
        fig.add_hrect(y0=0, y1=1, fillcolor="green", opacity=0.1, line_width=0)
        fig.add_hrect(y0=-1, y1=0, fillcolor="red", opacity=0.1, line_width=0)
        
        fig.update_layout(
            title='ê¸°ìˆ  ì´ìŠˆì— ëŒ€í•œ ê°ì • íŠ¸ë Œë“œ',
            xaxis_title='ë‚ ì§œ',
            yaxis_title='ê°ì • ì ìˆ˜ (-1: ë§¤ìš° ë¶€ì •ì , +1: ë§¤ìš° ê¸ì •ì )',
            hovermode='x unified'
        )
        
        return fig
    
    def create_topic_wordcloud_data(self) -> Dict:
        """ì›Œë“œí´ë¼ìš°ë“œìš© í† í”½ ë°ì´í„° ìƒì„±"""
        trending_topics = self.analyzer.get_trending_topics()
        
        wordcloud_data = {}
        for topic_name, topic_info in trending_topics['topics'].items():
            words = topic_info['words']
            weights = topic_info['weight']
            
            # ë‹¨ì–´-ê°€ì¤‘ì¹˜ ë”•ì…”ë„ˆë¦¬ ìƒì„±
            word_freq = dict(zip(words, weights))
            wordcloud_data[topic_name] = word_freq
        
        return wordcloud_data

# ë©”ì¸ ì‹¤í–‰ë¶€
if __name__ == "__main__":
    # ì˜ˆì‹œ ë°ì´í„°ë¡œ í…ŒìŠ¤íŠ¸
    sample_articles = [
        {
            "title": "ìŠ¤í•€íŠ¸ë¡œë‹‰ìŠ¤ ê¸°ìˆ ì˜ ìƒˆë¡œìš´ ëŒíŒŒêµ¬",
            "content": "ì—°êµ¬ì§„ì´ ìŠ¤í•€íŠ¸ë¡œë‹‰ìŠ¤ ê¸°ìˆ ì„ ì´ìš©í•´ ê¸°ì¡´ë³´ë‹¤ 100ë°° ë¹ ë¥¸ ì—°ì‚°ì†ë„ë¥¼ ë‹¬ì„±í–ˆë‹¤.",
            "country": "ë¯¸êµ­",
            "category": "í•˜ë“œì›¨ì–´ í˜ì‹ ",
            "date": "2025-07-01",
            "sentiment": 0.8,
            "source": "Tech Review"
        },
        {
            "title": "AI ê·œì œ ë²•ì•ˆ í†µê³¼, ì—…ê³„ ë°˜ë°œ",
            "content": "ìƒˆë¡œìš´ AI ê·œì œ ë²•ì•ˆì´ í†µê³¼ë˜ë©´ì„œ ê¸°ìˆ  ì—…ê³„ì—ì„œ ê°•í•œ ë°˜ë°œì´ ì¼ê³  ìˆë‹¤.",
            "country": "ë…ì¼",
            "category": "ë²•ë¥ /ê·œì œ", 
            "date": "2025-06-30",
            "sentiment": -0.3,
            "source": "Euro News"
        }
    ]
    
    analyzer = TechTrendAnalyzer(sample_articles)
    visualizer = AdvancedVisualizer(analyzer)
    
    # ë¶„ì„ ê²°ê³¼
    print(analyzer.generate_insights_report())
