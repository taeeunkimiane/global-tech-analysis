# app.py - ë©”ì¸ Streamlit ì• í”Œë¦¬ì¼€ì´ì…˜ (import ë¬¸ì œ í•´ê²° ë²„ì „)
import streamlit as st
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import numpy as np
from datetime import datetime, timedelta
import json
import os
from collections import Counter
import time

# ì„ íƒì  import - ì—†ì–´ë„ ì•±ì´ ì‘ë™í•˜ë„ë¡ ìˆ˜ì •
try:
    import requests
    from bs4 import BeautifulSoup
    import feedparser
    WEB_SCRAPING_AVAILABLE = True
except ImportError:
    WEB_SCRAPING_AVAILABLE = False
    st.warning("âš ï¸ ì›¹ ìŠ¤í¬ë˜í•‘ ëª¨ë“ˆì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ìƒ˜í”Œ ë°ì´í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")

try:
    from textblob import TextBlob
    TEXTBLOB_AVAILABLE = True
except ImportError:
    TEXTBLOB_AVAILABLE = False

try:
    import networkx as nx
    NETWORKX_AVAILABLE = True
except ImportError:
    NETWORKX_AVAILABLE = False

try:
    from sklearn.feature_extraction.text import TfidfVectorizer
    from sklearn.cluster import KMeans
    from sklearn.decomposition import LatentDirichletAllocation
    SKLEARN_AVAILABLE = True
except ImportError:
    SKLEARN_AVAILABLE = False

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="ê¸€ë¡œë²Œ ê¸°ìˆ  ì´ìŠˆ ë¶„ì„ ì‹œìŠ¤í…œ",
    page_icon="ğŸŒ",
    layout="wide",
    initial_sidebar_state="expanded"
)

# CSS ìŠ¤íƒ€ì¼ë§
st.markdown("""
<style>
    .main-header {
        font-size: 3rem;
        font-weight: bold;
        text-align: center;
        background: linear-gradient(90deg, #667eea 0%, #764ba2 100%);
        -webkit-background-clip: text;
        -webkit-text-fill-color: transparent;
        margin-bottom: 2rem;
    }
    .sub-header {
        font-size: 1.5rem;
        color: #2E86AB;
        margin-bottom: 1rem;
        border-bottom: 2px solid #E8F4FD;
        padding-bottom: 0.5rem;
    }
    .metric-card {
        background: linear-gradient(45deg, #f0f2f6, #ffffff);
        padding: 1.5rem;
        border-radius: 15px;
        border-left: 5px solid #667eea;
        margin: 0.5rem 0;
        box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
        transition: transform 0.3s ease;
    }
    .metric-card:hover {
        transform: translateY(-2px);
        box-shadow: 0 6px 12px rgba(0, 0, 0, 0.15);
    }
    .tech-category {
        background: linear-gradient(135deg, #667eea, #764ba2);
        color: white;
        padding: 0.5rem 1rem;
        border-radius: 20px;
        margin: 0.2rem;
        display: inline-block;
        font-size: 0.9rem;
        font-weight: bold;
    }
    .country-flag {
        font-size: 1.5rem;
        margin-right: 0.5rem;
    }
    .insight-box {
        background: linear-gradient(45deg, #e3f2fd, #f3e5f5);
        padding: 1.5rem;
        border-radius: 10px;
        border-left: 4px solid #2196f3;
        margin: 1rem 0;
    }
    .trend-up {
        color: #4caf50;
        font-weight: bold;
    }
    .trend-down {
        color: #f44336;
        font-weight: bold;
    }
    .data-source {
        font-size: 0.8rem;
        color: #666;
        text-align: right;
        margin-top: 1rem;
    }
</style>
""", unsafe_allow_html=True)

# ê¸°ìˆ  ì¹´í…Œê³ ë¦¬ ì •ì˜ (ë¬¸ì„œ ê¸°ë°˜)
TECH_CATEGORIES = {
    "í•˜ë“œì›¨ì–´ í˜ì‹ ": {
        "keywords": ["ìŠ¤í•€íŠ¸ë¡œë‹‰ìŠ¤", "ë§ˆê·¸ë…¸ë‹‰ìŠ¤", "ì–‘ìì»´í“¨íŒ…", "ë°˜ë„ì²´", "ì¹©ì…‹", "í”„ë¡œì„¸ì„œ"],
        "color": "#FF6B6B"
    },
    "AI/ML": {
        "keywords": ["ì¸ê³µì§€ëŠ¥", "ë¨¸ì‹ ëŸ¬ë‹", "ë”¥ëŸ¬ë‹", "GPT", "LLM", "ë² ì´ì§€ì•ˆ"],
        "color": "#4ECDC4"
    },
    "ë³´ì•ˆ/í•´í‚¹": {
        "keywords": ["ì‚¬ì´ë²„ë³´ì•ˆ", "í•´í‚¹", "ë°ì´í„°ë³´ì•ˆ", "í”„ë¼ì´ë²„ì‹œ", "ì•”í˜¸í™”"],
        "color": "#45B7D1"
    },
    "ë²•ë¥ /ê·œì œ": {
        "keywords": ["AIë²•", "ë°ì´í„°ë³´í˜¸", "GDPR", "ê·œì œ", "ì •ì±…", "ê±°ë²„ë„ŒìŠ¤"],
        "color": "#96CEB4"
    },
    "ììœ¨ì‹œìŠ¤í…œ": {
        "keywords": ["ììœ¨ì£¼í–‰", "ë¡œë´‡", "ë“œë¡ ", "IoT", "ìŠ¤ë§ˆíŠ¸ì‹œí‹°"],
        "color": "#FFEAA7"
    }
}

COUNTRIES = {
    "ë¯¸êµ­": "ğŸ‡ºğŸ‡¸",
    "ì¤‘êµ­": "ğŸ‡¨ğŸ‡³", 
    "ì¼ë³¸": "ğŸ‡¯ğŸ‡µ",
    "ë…ì¼": "ğŸ‡©ğŸ‡ª",
    "ì˜êµ­": "ğŸ‡¬ğŸ‡§",
    "í”„ë‘ìŠ¤": "ğŸ‡«ğŸ‡·",
    "í•œêµ­": "ğŸ‡°ğŸ‡·",
    "ì´ìŠ¤ë¼ì—˜": "ğŸ‡®ğŸ‡±",
    "ì‹±ê°€í¬ë¥´": "ğŸ‡¸ğŸ‡¬",
    "ìºë‚˜ë‹¤": "ğŸ‡¨ğŸ‡¦"
}

# ===== ì›ë˜ external ëª¨ë“ˆë“¤ì„ ë‚´ì¥ í•¨ìˆ˜ë¡œ ë³€í™˜ =====

class TechNewsAnalyzer:
    """ê¸°ìˆ  ë‰´ìŠ¤ ë¶„ì„ê¸° (ë‚´ì¥ ë²„ì „)"""
    
    def __init__(self, articles):
        self.articles = articles
        self.df = pd.DataFrame(articles) if articles else pd.DataFrame()
        if not self.df.empty:
            self.df['date'] = pd.to_datetime(self.df['date'])
            # í…ìŠ¤íŠ¸ ê²°í•©
            self.df['full_text'] = self.df['title'] + ' ' + self.df['content']
            # í…ìŠ¤íŠ¸ ì •ë¦¬
            self.df['clean_text'] = self.df['full_text'].apply(self._clean_text)
            # ë‚ ì§œ ê´€ë ¨ í”¼ì²˜ ì¶”ê°€
            self.df['year'] = self.df['date'].dt.year
            self.df['month'] = self.df['date'].dt.month
            self.df['day_of_week'] = self.df['date'].dt.day_name()
    
    def _clean_text(self, text):
        """í…ìŠ¤íŠ¸ ì •ë¦¬"""
        import re
        # HTML íƒœê·¸ ì œê±°
        text = re.sub(r'<[^>]+>', '', text)
        # íŠ¹ìˆ˜ë¬¸ì ì œê±° (ë‹¨, í•œê¸€, ì˜ë¬¸, ìˆ«ìëŠ” ìœ ì§€)
        text = re.sub(r'[^\w\sê°€-í£]', ' ', text)
        # ì—¬ëŸ¬ ê³µë°±ì„ í•˜ë‚˜ë¡œ
        text = re.sub(r'\s+', ' ', text)
        return text.strip().lower()

    def get_trending_topics(self, n_topics=5):
        """í† í”½ ëª¨ë¸ë§ì„ í†µí•œ íŠ¸ë Œë”© ì£¼ì œ ë¶„ì„"""
        if not SKLEARN_AVAILABLE or self.df.empty:
            # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê¸°ë°˜ ë¶„ì„ìœ¼ë¡œ ëŒ€ì²´
            return self._simple_topic_analysis()
        
        try:
            # TF-IDF ë²¡í„°í™”
            vectorizer = TfidfVectorizer(
                max_features=1000,
                stop_words='english',
                ngram_range=(1, 2),
                min_df=2,
                max_df=0.8
            )
            
            tfidf_matrix = vectorizer.fit_transform(self.df['clean_text'])
            
            # LDA í† í”½ ëª¨ë¸ë§
            lda = LatentDirichletAllocation(
                n_components=n_topics,
                random_state=42,
                max_iter=10
            )
            
            lda.fit(tfidf_matrix)
            
            # í† í”½ë³„ ì£¼ìš” ë‹¨ì–´ ì¶”ì¶œ
            feature_names = vectorizer.get_feature_names_out()
            topics = {}
            
            for topic_idx, topic in enumerate(lda.components_):
                top_words_idx = topic.argsort()[-10:][::-1]
                top_words = [feature_names[i] for i in top_words_idx]
                topics[f"Topic_{topic_idx + 1}"] = {
                    'words': top_words,
                    'weight': topic[top_words_idx].tolist()
                }
            
            # ë¬¸ì„œ-í† í”½ ë¶„í¬
            doc_topic_dist = lda.transform(tfidf_matrix)
            
            return {
                'topics': topics,
                'doc_topic_distribution': doc_topic_dist,
                'vectorizer': vectorizer,
                'lda_model': lda
            }
        except Exception as e:
            return self._simple_topic_analysis()
    
    def _simple_topic_analysis(self):
        """ê°„ë‹¨í•œ í† í”½ ë¶„ì„ (sklearn ì—†ì„ ë•Œ)"""
        topics = {}
        for i, (category, info) in enumerate(TECH_CATEGORIES.items()):
            topics[f"Topic_{i+1}"] = {
                'words': info['keywords'][:5],
                'weight': [1.0, 0.8, 0.6, 0.4, 0.2]
            }
        
        return {
            'topics': topics,
            'doc_topic_distribution': None,
            'vectorizer': None,
            'lda_model': None
        }

    def analyze_country_tech_focus(self):
        """êµ­ê°€ë³„ ê¸°ìˆ  ì§‘ì¤‘ë„ ë¶„ì„"""
        if self.df.empty:
            return {}
        
        from collections import defaultdict
        country_tech = defaultdict(lambda: defaultdict(int))
        
        for _, row in self.df.iterrows():
            country_tech[row['country']][row['category']] += 1
        
        # ì •ê·œí™” (ê° êµ­ê°€ì˜ ì´ ê¸°ì‚¬ ìˆ˜ë¡œ ë‚˜ëˆ„ê¸°)
        country_tech_normalized = {}
        for country, tech_counts in country_tech.items():
            total_articles = sum(tech_counts.values())
            country_tech_normalized[country] = {
                tech: count / total_articles
                for tech, count in tech_counts.items()
            }
        
        return country_tech_normalized

    def detect_emerging_trends(self, window_days=7):
        """ì‹ í¥ íŠ¸ë Œë“œ ê°ì§€"""
        if self.df.empty:
            return {}
        
        recent_date = self.df['date'].max()
        cutoff_date = recent_date - timedelta(days=window_days)
        
        recent_articles = self.df[self.df['date'] >= cutoff_date]
        older_articles = self.df[self.df['date'] < cutoff_date]
        
        # ìµœê·¼ ê¸°ê°„ê³¼ ì´ì „ ê¸°ê°„ì˜ í‚¤ì›Œë“œ ë¹ˆë„ ë¹„êµ
        recent_keywords = self._extract_keywords(recent_articles['clean_text'] if not recent_articles.empty else pd.Series())
        older_keywords = self._extract_keywords(older_articles['clean_text'] if not older_articles.empty else pd.Series())
        
        # íŠ¸ë Œë“œ ìŠ¤ì½”ì–´ ê³„ì‚° (ìµœê·¼ ë¹ˆë„ / ì´ì „ ë¹ˆë„)
        trending_keywords = {}
        for keyword, recent_freq in recent_keywords.items():
            older_freq = older_keywords.get(keyword, 1)  # 0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ë°©ì§€
            trend_score = recent_freq / older_freq
            
            if recent_freq >= 3 and trend_score > 2:  # ìµœì†Œ ë¹ˆë„ì™€ íŠ¸ë Œë“œ ìŠ¤ì½”ì–´ ê¸°ì¤€
                trending_keywords[keyword] = {
                    'recent_frequency': recent_freq,
                    'older_frequency': older_freq,
                    'trend_score': trend_score
                }
        
        # íŠ¸ë Œë“œ ìŠ¤ì½”ì–´ ìˆœìœ¼ë¡œ ì •ë ¬
        trending_keywords = dict(
            sorted(trending_keywords.items(), 
                   key=lambda x: x[1]['trend_score'], 
                   reverse=True)
        )
        
        return trending_keywords

    def _extract_keywords(self, texts, n_keywords=50):
        """í…ìŠ¤íŠ¸ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        if texts.empty:
            return {}
        
        if SKLEARN_AVAILABLE:
            try:
                vectorizer = TfidfVectorizer(
                    max_features=n_keywords,
                    stop_words='english',
                    ngram_range=(1, 2),
                    min_df=2
                )
                
                tfidf_matrix = vectorizer.fit_transform(texts)
                feature_names = vectorizer.get_feature_names_out()
                
                # ê° í‚¤ì›Œë“œì˜ ì´ TF-IDF ì ìˆ˜ ê³„ì‚°
                keyword_scores = {}
                for i, keyword in enumerate(feature_names):
                    keyword_scores[keyword] = tfidf_matrix[:, i].sum()
                
                return dict(Counter(keyword_scores).most_common(n_keywords))
            except:
                pass
        
        # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ì¶”ì¶œ (fallback)
        all_keywords = []
        for category_info in TECH_CATEGORIES.values():
            all_keywords.extend(category_info['keywords'])
        
        keyword_counts = {}
        combined_text = ' '.join(texts.fillna(''))
        
        for keyword in all_keywords:
            count = combined_text.lower().count(keyword.lower())
            if count > 0:
                keyword_counts[keyword] = count
        
        return keyword_counts

    def sentiment_trend_analysis(self):
        """ê°ì • íŠ¸ë Œë“œ ë¶„ì„"""
        if self.df.empty:
            return {
                'daily_trend': pd.DataFrame(),
                'country_sentiment': pd.DataFrame(),
                'category_sentiment': pd.DataFrame()
            }
        
        # ë‚ ì§œë³„ í‰ê·  ê°ì • ê³„ì‚°
        daily_sentiment = self.df.groupby('date')['sentiment'].agg(['mean', 'count']).reset_index()
        
        # êµ­ê°€ë³„ ê°ì • ë¶„ì„
        country_sentiment = self.df.groupby('country')['sentiment'].agg(['mean', 'std', 'count']).reset_index()
        
        # ì¹´í…Œê³ ë¦¬ë³„ ê°ì • ë¶„ì„
        category_sentiment = self.df.groupby('category')['sentiment'].agg(['mean', 'std', 'count']).reset_index()
        
        return {
            'daily_trend': daily_sentiment,
            'country_sentiment': country_sentiment,
            'category_sentiment': category_sentiment
        }

    def create_technology_network(self):
        """ê¸°ìˆ  ê°„ ì—°ê´€ì„± ë„¤íŠ¸ì›Œí¬ ìƒì„±"""
        if not NETWORKX_AVAILABLE:
            return None
        
        try:
            from collections import defaultdict
            import itertools
            
            # ê¸°ì‚¬ë³„ë¡œ ì–¸ê¸‰ëœ ê¸°ìˆ ë“¤ì„ ì¶”ì¶œ
            tech_cooccurrence = defaultdict(lambda: defaultdict(int))
            
            # ê¸°ìˆ  í‚¤ì›Œë“œ ì •ì˜ (í™•ì¥ëœ ë²„ì „)
            tech_keywords = {
                'ìŠ¤í•€íŠ¸ë¡œë‹‰ìŠ¤': ['spintronics', 'ìŠ¤í•€íŠ¸ë¡œë‹‰ìŠ¤', 'spin electronics'],
                'ì–‘ìì»´í“¨íŒ…': ['quantum computing', 'ì–‘ìì»´í“¨íŒ…', 'quantum computer'],
                'AI': ['artificial intelligence', 'ì¸ê³µì§€ëŠ¥', 'machine learning', 'deep learning'],
                'ììœ¨ì£¼í–‰': ['autonomous vehicle', 'ììœ¨ì£¼í–‰', 'self-driving'],
                'ë¸”ë¡ì²´ì¸': ['blockchain', 'ë¸”ë¡ì²´ì¸', 'cryptocurrency'],
                '5G': ['5G', '5g network', '5ì„¸ëŒ€'],
                'IoT': ['IoT', 'internet of things', 'ì‚¬ë¬¼ì¸í„°ë„·'],
                'AR/VR': ['augmented reality', 'virtual reality', 'AR', 'VR', 'ì¦ê°•í˜„ì‹¤', 'ê°€ìƒí˜„ì‹¤']
            }
            
            # ê° ê¸°ì‚¬ì—ì„œ ì–¸ê¸‰ëœ ê¸°ìˆ ë“¤ ì°¾ê¸°
            for _, row in self.df.iterrows():
                text = row['clean_text']
                mentioned_techs = []
                
                for tech_name, keywords in tech_keywords.items():
                    if any(keyword.lower() in text for keyword in keywords):
                        mentioned_techs.append(tech_name)
                
                # ë™ì‹œ ì¶œí˜„ ê¸°ë¡
                for i, tech1 in enumerate(mentioned_techs):
                    for tech2 in mentioned_techs[i+1:]:
                        tech_cooccurrence[tech1][tech2] += 1
                        tech_cooccurrence[tech2][tech1] += 1
            
            # ë„¤íŠ¸ì›Œí¬ ê·¸ë˜í”„ ìƒì„±
            G = nx.Graph()
            
            for tech1, connections in tech_cooccurrence.items():
                for tech2, weight in connections.items():
                    if weight >= 2:  # ìµœì†Œ 2ë²ˆ ì´ìƒ ë™ì‹œ ì¶œí˜„
                        G.add_edge(tech1, tech2, weight=weight)
            
            return G
        except Exception as e:
            return None

    def generate_insights_report(self):
        """ì¸ì‚¬ì´íŠ¸ ë³´ê³ ì„œ ìƒì„±"""
        if self.df.empty:
            return "ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."
        
        report = []
        
        # ê¸°ë³¸ í†µê³„
        total_articles = len(self.df)
        date_range = f"{self.df['date'].min().strftime('%Y-%m-%d')} ~ {self.df['date'].max().strftime('%Y-%m-%d')}"
        
        report.append(f"ğŸ“Š **ë¶„ì„ ê¸°ê°„**: {date_range}")
        report.append(f"ğŸ“° **ì´ ê¸°ì‚¬ ìˆ˜**: {total_articles:,}ê°œ")
        report.append("")
        
        # êµ­ê°€ë³„ í™œë™ë„
        country_counts = self.df['country'].value_counts()
        report.append("ğŸŒ **êµ­ê°€ë³„ ê¸°ìˆ  ì´ìŠˆ í™œë™ë„**")
        for country, count in country_counts.head(5).items():
            percentage = (count / total_articles) * 100
            flag = COUNTRIES.get(country, "ğŸ³ï¸")
            report.append(f"- {flag} {country}: {count}ê°œ ({percentage:.1f}%)")
        report.append("")
        
        # ê¸°ìˆ  ë¶„ì•¼ë³„ ê´€ì‹¬ë„
        category_counts = self.df['category'].value_counts()
        report.append("ğŸ”¬ **ê¸°ìˆ  ë¶„ì•¼ë³„ ê´€ì‹¬ë„**")
        for category, count in category_counts.head(5).items():
            percentage = (count / total_articles) * 100
            report.append(f"- {category}: {count}ê°œ ({percentage:.1f}%)")
        report.append("")
        
        # ê°ì • ë¶„ì„ ê²°ê³¼
        avg_sentiment = self.df['sentiment'].mean()
        sentiment_label = "ê¸ì •ì " if avg_sentiment > 0.1 else "ë¶€ì •ì " if avg_sentiment < -0.1 else "ì¤‘ë¦½ì "
        report.append("ğŸ­ **ì „ë°˜ì  ì—¬ë¡ **")
        report.append(f"- í‰ê·  ê°ì • ì ìˆ˜: {avg_sentiment:.3f} ({sentiment_label})")
        report.append("")
        
        # ì‹ í¥ íŠ¸ë Œë“œ
        trending = self.detect_emerging_trends()
        if trending:
            report.append("ğŸ“ˆ **ì‹ í¥ íŠ¸ë Œë“œ í‚¤ì›Œë“œ** (ìµœê·¼ 7ì¼)")
            for keyword, data in list(trending.items())[:5]:
                report.append(f"- {keyword}: íŠ¸ë Œë“œ ì ìˆ˜ {data['trend_score']:.1f}x")
            report.append("")
        
        # ê¸°ìˆ  ì§‘ì¤‘ë„ê°€ ë†’ì€ êµ­ê°€
        country_focus = self.analyze_country_tech_focus()
        report.append("ğŸ¯ **êµ­ê°€ë³„ ê¸°ìˆ  íŠ¹í™” ë¶„ì•¼**")
        for country, tech_dist in list(country_focus.items())[:5]:
            if tech_dist:
                top_tech = max(tech_dist, key=tech_dist.get)
                focus_rate = tech_dist[top_tech]
                flag = COUNTRIES.get(country, "ğŸ³ï¸")
                report.append(f"- {flag} {country}: {top_tech} ({focus_rate:.1%} ì§‘ì¤‘)")
        
        return "\n".join(report)

class AdvancedVisualizer:
    """ê³ ê¸‰ ì‹œê°í™” í´ë˜ìŠ¤ (ë‚´ì¥ ë²„ì „)"""
    
    def __init__(self, analyzer):
        self.analyzer = analyzer
        self.df = analyzer.df
    
    def create_network_visualization(self):
        """ê¸°ìˆ  ë„¤íŠ¸ì›Œí¬ ì‹œê°í™”"""
        G = self.analyzer.create_technology_network()
        
        if G is None or len(G.nodes()) == 0:
            # ë¹ˆ ê·¸ë˜í”„ì¸ ê²½ìš°
            fig = go.Figure()
            fig.add_annotation(
                text="ì¶©ë¶„í•œ ê¸°ìˆ  ì—°ê´€ì„± ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤." + ("<br>NetworkXê°€ í•„ìš”í•©ë‹ˆë‹¤." if not NETWORKX_AVAILABLE else ""),
                xref="paper", yref="paper",
                x=0.5, y=0.5, xanchor='center', yanchor='middle',
                showarrow=False, font=dict(size=16)
            )
            return fig
        
        # ë…¸ë“œ ìœ„ì¹˜ ê³„ì‚°
        pos = nx.spring_layout(G, k=3, iterations=50)
        
        # ì—£ì§€ ê·¸ë¦¬ê¸°
        edge_x = []
        edge_y = []
        edge_info = []
        
        for edge in G.edges():
            x0, y0 = pos[edge[0]]
            x1, y1 = pos[edge[1]]
            edge_x.extend([x0, x1, None])
            edge_y.extend([y0, y1, None])
            weight = G[edge[0]][edge[1]]['weight']
            edge_info.append(f"{edge[0]} â†” {edge[1]}: {weight}ê°œ ê¸°ì‚¬ì—ì„œ í•¨ê»˜ ì–¸ê¸‰")
        
        edge_trace = go.Scatter(
            x=edge_x, y=edge_y,
            line=dict(width=2, color='#888'),
            hoverinfo='none',
            mode='lines'
        )
        
        # ë…¸ë“œ ê·¸ë¦¬ê¸°
        node_x = []
        node_y = []
        node_text = []
        node_info = []
        node_size = []
        
        for node in G.nodes():
            x, y = pos[node]
            node_x.append(x)
            node_y.append(y)
            node_text.append(node)
            
            # ë…¸ë“œ í¬ê¸°ëŠ” ì—°ê²°ëœ ì—£ì§€ ìˆ˜ì— ë¹„ë¡€
            connections = len(list(G.neighbors(node)))
            node_size.append(20 + connections * 10)
            node_info.append(f"{node}<br>ì—°ê²°ëœ ê¸°ìˆ : {connections}ê°œ")
        
        node_trace = go.Scatter(
            x=node_x, y=node_y,
            mode='markers+text',
            hoverinfo='text',
            text=node_text,
            textposition="middle center",
            hovertext=node_info,
            marker=dict(
                size=node_size,
                color='lightblue',
                line=dict(width=2, color='darkblue')
            )
        )
        
        fig = go.Figure(data=[edge_trace, node_trace],
                       layout=go.Layout(
                           title='ê¸°ìˆ  ê°„ ì—°ê´€ì„± ë„¤íŠ¸ì›Œí¬',
                           titlefont_size=16,
                           showlegend=False,
                           hovermode='closest',
                           margin=dict(b=20,l=5,r=5,t=40),
                           annotations=[ dict(
                               text="ë…¸ë“œ í¬ê¸°ëŠ” ë‹¤ë¥¸ ê¸°ìˆ ê³¼ì˜ ì—°ê´€ì„± ì •ë„ë¥¼ ë‚˜íƒ€ëƒ„",
                               showarrow=False,
                               xref="paper", yref="paper",
                               x=0.005, y=-0.002 ) ],
                           xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),
                           yaxis=dict(showgrid=False, zeroline=False, showticklabels=False)
                       ))
        
        return fig
    
    def create_sentiment_timeline(self):
        """ê°ì • íŠ¸ë Œë“œ íƒ€ì„ë¼ì¸"""
        sentiment_data = self.analyzer.sentiment_trend_analysis()
        daily_sentiment = sentiment_data['daily_trend']
        
        if daily_sentiment.empty:
            fig = go.Figure()
            fig.add_annotation(
                text="ê°ì • íŠ¸ë Œë“œ ë°ì´í„°ê°€ ì¶©ë¶„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.",
                xref="paper", yref="paper",
                x=0.5, y=0.5, xanchor='center', yanchor='middle',
                showarrow=False, font=dict(size=16)
            )
            return fig
        
        fig = go.Figure()
        
        # ì „ì²´ ê°ì • íŠ¸ë Œë“œ
        fig.add_trace(go.Scatter(
            x=daily_sentiment['date'],
            y=daily_sentiment['mean'],
            mode='lines+markers',
            name='ì¼í‰ê·  ê°ì •',
            line=dict(color='blue', width=3),
            hovertemplate='%{x}<br>ê°ì • ì ìˆ˜: %{y:.3f}<br>ê¸°ì‚¬ ìˆ˜: %{customdata}<extra></extra>',
            customdata=daily_sentiment['count']
        ))
        
        # 0ì„  ì¶”ê°€
        fig.add_hline(y=0, line_dash="dash", line_color="gray", 
                     annotation_text="ì¤‘ë¦½ì„ ", annotation_position="bottom right")
        
        # ê¸ì •/ë¶€ì • ì˜ì—­ ìƒ‰ì¹ 
        fig.add_hrect(y0=0, y1=1, fillcolor="green", opacity=0.1, line_width=0)
        fig.add_hrect(y0=-1, y1=0, fillcolor="red", opacity=0.1, line_width=0)
        
        fig.update_layout(
            title='ê¸°ìˆ  ì´ìŠˆì— ëŒ€í•œ ê°ì • íŠ¸ë Œë“œ',
            xaxis_title='ë‚ ì§œ',
            yaxis_title='ê°ì • ì ìˆ˜ (-1: ë§¤ìš° ë¶€ì •ì , +1: ë§¤ìš° ê¸ì •ì )',
            hovermode='x unified'
        )
        
        return fig

# ë°ì´í„° ìºì‹±ì„ ìœ„í•œ í•¨ìˆ˜ë“¤
@st.cache_data(ttl=3600)  # 1ì‹œê°„ ìºì‹œ
def load_cached_data():
    """ìºì‹œëœ ë°ì´í„° ë¡œë“œ"""
    try:
        with open('tech_news_data.json', 'r', encoding='utf-8') as f:
            return json.load(f)
    except FileNotFoundError:
        return None

@st.cache_data(ttl=1800)  # 30ë¶„ ìºì‹œ
def get_sample_data():
    """ìƒ˜í”Œ ë°ì´í„° ìƒì„± (ì‹¤ì œ ë°ì´í„°ê°€ ì—†ì„ ë•Œ)"""
    countries = list(COUNTRIES.keys())
    categories = list(TECH_CATEGORIES.keys())
    
    sample_articles = []
    base_date = datetime.now() - timedelta(days=30)
    
    # ì‹¤ì œì ì¸ ìƒ˜í”Œ ê¸°ì‚¬ë“¤
    realistic_articles = [
        {
            'title': 'ë¯¸êµ­, ì°¨ì„¸ëŒ€ ìŠ¤í•€íŠ¸ë¡œë‹‰ìŠ¤ ì¹© ê°œë°œ ì„±ê³µ',
            'content': 'ë¯¸êµ­ ì—°êµ¬ì§„ì´ ì „ìì˜ ìŠ¤í•€ì„ ì´ìš©í•œ í˜ì‹ ì ì¸ ì»´í“¨íŒ… ê¸°ìˆ ì„ ê°œë°œí–ˆë‹¤. ì´ ê¸°ìˆ ì€ ê¸°ì¡´ ë°˜ë„ì²´ë³´ë‹¤ 100ë°° ë¹ ë¥¸ ì—°ì‚° ì†ë„ì™€ 1/10 ìˆ˜ì¤€ì˜ ì „ë ¥ ì†Œë¹„ë¥¼ ì‹¤í˜„í•œë‹¤.',
            'country': 'ë¯¸êµ­',
            'category': 'í•˜ë“œì›¨ì–´ í˜ì‹ ',
            'date': '2025-07-03',
            'sentiment': 0.752,
            'source': 'MIT Technology Review',
            'url': 'https://example.com/article/1',
            'scraped_at': datetime.now().isoformat()
        },
        {
            'title': 'ì¤‘êµ­ AI ê¸°ì—…, ë¹„ëª¨ìˆ˜ ë² ì´ì§€ì•ˆ ëª¨ë¸ ê°œë°œ',
            'content': 'ì¤‘êµ­ AI ê¸°ì—…ì´ ë¶ˆí™•ì‹¤ì„±ì„ ì •ëŸ‰ì ìœ¼ë¡œ í‘œí˜„í•  ìˆ˜ ìˆëŠ” ë¹„ëª¨ìˆ˜ ë² ì´ì§€ì•ˆ AI ëª¨ë¸ì„ ê°œë°œí–ˆë‹¤ê³  ë°œí‘œí–ˆë‹¤.',
            'country': 'ì¤‘êµ­',
            'category': 'AI/ML',
            'date': '2025-07-02',
            'sentiment': 0.634,
            'source': 'Tech in Asia',
            'url': 'https://example.com/article/2',
            'scraped_at': datetime.now().isoformat()
        },
        {
            'title': 'ì¼ë³¸ ì •ë¶€ê¸°ê´€, ëŒ€ê·œëª¨ ì‚¬ì´ë²„ ê³µê²© ë°›ì•„',
            'content': 'ì¼ë³¸ì˜ ì£¼ìš” ì •ë¶€ê¸°ê´€ì´ AI ì‹œìŠ¤í…œì„ ëŒ€ìƒìœ¼ë¡œ í•œ ì ëŒ€ì  ê³µê²©ì„ ë°›ì•˜ë‹¤ê³  ë°œí‘œí–ˆë‹¤.',
            'country': 'ì¼ë³¸',
            'category': 'ë³´ì•ˆ/í•´í‚¹',
            'date': '2025-07-01',
            'sentiment': -0.423,
            'source': 'Nikkei Asia',
            'url': 'https://example.com/article/3',
            'scraped_at': datetime.now().isoformat()
        },
        {
            'title': 'ë…ì¼, AI ê·œì œ ë²•ì•ˆ ì˜íšŒ í†µê³¼',
            'content': 'ë…ì¼ ì˜íšŒê°€ AI ì‹œìŠ¤í…œì˜ ì•ˆì „ì„±ê³¼ íˆ¬ëª…ì„±ì„ ë³´ì¥í•˜ê¸° ìœ„í•œ í¬ê´„ì ì¸ ë²•ì  í”„ë ˆì„ì›Œí¬ë¥¼ í†µê³¼ì‹œì¼°ë‹¤.',
            'country': 'ë…ì¼',
            'category': 'ë²•ë¥ /ê·œì œ',
            'date': '2025-06-30',
            'sentiment': 0.156,
            'source': 'The Next Web',
            'url': 'https://example.com/article/4',
            'scraped_at': datetime.now().isoformat()
        },
        {
            'title': 'í•œêµ­ ìë™ì°¨ ì—…ì²´, ì™„ì „ììœ¨ì£¼í–‰ ê¸°ìˆ  ì‹œì—°',
            'content': 'í•œêµ­ì˜ ì£¼ìš” ìë™ì°¨ ì œì¡°ì‚¬ê°€ ë§ˆê·¸ë…¸ë‹‰ìŠ¤ ê¸°ìˆ ì„ ì ìš©í•œ ì™„ì „ììœ¨ì£¼í–‰ ì‹œìŠ¤í…œì„ ì„±ê³µì ìœ¼ë¡œ ì‹œì—°í–ˆë‹¤.',
            'country': 'í•œêµ­',
            'category': 'ììœ¨ì‹œìŠ¤í…œ',
            'date': '2025-06-29',
            'sentiment': 0.687,
            'source': 'TechCrunch',
            'url': 'https://example.com/article/5',
            'scraped_at': datetime.now().isoformat()
        }
    ]
    
    sample_articles.extend(realistic_articles)
    
    # ì¶”ê°€ ìƒ˜í”Œ ë°ì´í„° ìƒì„±
    for i in range(50):
        country = countries[i % len(countries)]
        category = categories[i % len(categories)]
        
        # ê°ì •ì ìˆ˜ëŠ” ì¹´í…Œê³ ë¦¬ì— ë”°ë¼ ë‹¤ë¥´ê²Œ ì„¤ì •
        if category == "ë³´ì•ˆ/í•´í‚¹":
            sentiment = np.random.normal(-0.2, 0.3)
        elif category == "ë²•ë¥ /ê·œì œ":
            sentiment = np.random.normal(-0.1, 0.4)
        else:
            sentiment = np.random.normal(0.2, 0.3)
        
        sentiment = np.clip(sentiment, -1, 1)
        
        article = {
            'title': f'{country} {category} ê´€ë ¨ ê¸°ìˆ  ë‰´ìŠ¤ {i+6}',
            'content': f'ì´ê²ƒì€ {category} ë¶„ì•¼ì˜ {country} ê´€ë ¨ ê¸°ì‚¬ì…ë‹ˆë‹¤. ìµœì‹  ê¸°ìˆ  ë™í–¥ê³¼ ë°œì „ì‚¬í•­ì„ ë‹¤ë£¨ê³  ìˆìŠµë‹ˆë‹¤.',
            'country': country,
            'category': category,
            'date': (base_date + timedelta(days=np.random.randint(0, 30))).strftime('%Y-%m-%d'),
            'sentiment': round(sentiment, 3),
            'source': f'{country} Tech News',
            'url': f'https://example.com/article/{i+6}',
            'scraped_at': datetime.now().isoformat()
        }
        sample_articles.append(article)
    
    return sample_articles

def main():
    # í—¤ë”
    st.markdown('<h1 class="main-header">ğŸŒ ê¸€ë¡œë²Œ ê¸°ìˆ  ì´ìŠˆ ë¶„ì„ ì‹œìŠ¤í…œ</h1>', unsafe_allow_html=True)
    st.markdown('<p style="text-align: center; font-size: 1.2rem; color: #666; margin-bottom: 2rem;">AI ì‹œëŒ€ì˜ ê¸°ìˆ  íŒ¨ëŸ¬ë‹¤ì„ ë³€í™”ì™€ êµ­ì œì  ê±°ë²„ë„ŒìŠ¤ ë™í–¥ ë¶„ì„</p>', unsafe_allow_html=True)
    
    # ì‚¬ì´ë“œë°” ì„¤ì •
    with st.sidebar:
        st.markdown("## ğŸ”§ ë¶„ì„ ì„¤ì •")
        
        # ë°°í¬ ìƒíƒœ í‘œì‹œ
        st.success("ğŸ‰ ì„±ê³µì ìœ¼ë¡œ ë°°í¬ë˜ì—ˆìŠµë‹ˆë‹¤!")
        
        # ê¸°ëŠ¥ ìƒíƒœ í‘œì‹œ
        st.markdown("### ğŸ“Š ê¸°ëŠ¥ ìƒíƒœ")
        st.write(f"âœ… ê¸°ë³¸ ë¶„ì„: ì‚¬ìš© ê°€ëŠ¥")
        st.write(f"{'âœ…' if TEXTBLOB_AVAILABLE else 'âš ï¸'} ê°ì • ë¶„ì„: {'ê³ ê¸‰' if TEXTBLOB_AVAILABLE else 'ê¸°ë³¸'}")
        st.write(f"{'âœ…' if NETWORKX_AVAILABLE else 'âš ï¸'} ë„¤íŠ¸ì›Œí¬ ë¶„ì„: {'ì‚¬ìš© ê°€ëŠ¥' if NETWORKX_AVAILABLE else 'ì œí•œë¨'}")
        st.write(f"{'âœ…' if SKLEARN_AVAILABLE else 'âš ï¸'} ê³ ê¸‰ ë¶„ì„: {'ì‚¬ìš© ê°€ëŠ¥' if SKLEARN_AVAILABLE else 'ì œí•œë¨'}")
        st.write(f"{'âœ…' if WEB_SCRAPING_AVAILABLE else 'âš ï¸'} ì›¹ ìŠ¤í¬ë˜í•‘: {'ì‚¬ìš© ê°€ëŠ¥' if WEB_SCRAPING_AVAILABLE else 'ì œí•œë¨'}")
        
        # ë°ì´í„° ì†ŒìŠ¤ ì„ íƒ
        data_source = st.radio(
            "ë°ì´í„° ì†ŒìŠ¤",
            ["ì‹¤ì‹œê°„ ìˆ˜ì§‘", "ìºì‹œëœ ë°ì´í„°", "ìƒ˜í”Œ ë°ì´í„°"],
            index=2,  # ê¸°ë³¸ê°’: ìƒ˜í”Œ ë°ì´í„°
            help="ì‹¤ì‹œê°„ ìˆ˜ì§‘ì€ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
        )
        
        # ë‚ ì§œ ë²”ìœ„ ì„ íƒ
        date_range = st.date_input(
            "ë¶„ì„ ê¸°ê°„",
            value=(datetime.now() - timedelta(days=30), datetime.now()),
            max_value=datetime.now(),
            help="ë¶„ì„í•  ê¸°ê°„ì„ ì„ íƒí•´ì£¼ì„¸ìš”."
        )
        
        # êµ­ê°€ ì„ íƒ
        selected_countries = st.multiselect(
            "ë¶„ì„ ëŒ€ìƒ êµ­ê°€",
            list(COUNTRIES.keys()),
            default=list(COUNTRIES.keys())[:5],
            help="ë¶„ì„í•  êµ­ê°€ë“¤ì„ ì„ íƒí•´ì£¼ì„¸ìš”."
        )
        
        # ê¸°ìˆ  ì¹´í…Œê³ ë¦¬ ì„ íƒ
        selected_categories = st.multiselect(
            "ê¸°ìˆ  ì¹´í…Œê³ ë¦¬",
            list(TECH_CATEGORIES.keys()),
            default=list(TECH_CATEGORIES.keys()),
            help="ë¶„ì„í•  ê¸°ìˆ  ë¶„ì•¼ë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”."
        )
        
        st.markdown("---")
        
        # ë°ì´í„° ê°±ì‹ 
        st.markdown("### ğŸ“Š ë°ì´í„° ê´€ë¦¬")
        
        if st.button("ğŸ”„ ë°ì´í„° ê°±ì‹ ", type="primary"):
            if data_source == "ì‹¤ì‹œê°„ ìˆ˜ì§‘":
                if WEB_SCRAPING_AVAILABLE:
                    with st.spinner("ë‰´ìŠ¤ ë°ì´í„° ìˆ˜ì§‘ ì¤‘... (ì•½ 2-3ë¶„ ì†Œìš”)"):
                        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” scraper.scrape_all_sources() ì‚¬ìš©
                        st.warning("ì‹¤ì‹œê°„ ìˆ˜ì§‘ ê¸°ëŠ¥ì€ í˜„ì¬ ê°œë°œ ì¤‘ì…ë‹ˆë‹¤. ìƒ˜í”Œ ë°ì´í„°ë¥¼ ì‚¬ìš©í•´ì£¼ì„¸ìš”.")
                else:
                    st.error("ì›¹ ìŠ¤í¬ë˜í•‘ ëª¨ë“ˆì´ ì—†ì–´ ì‹¤ì‹œê°„ ìˆ˜ì§‘ì„ í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            else:
                st.success("âœ… ë°ì´í„° ê°±ì‹  ì™„ë£Œ!")
                st.rerun()
    
    # ë©”ì¸ ì»¨í…ì¸  - ë°ì´í„° ë¡œë“œ
    with st.spinner("ë°ì´í„° ë¡œë”© ì¤‘..."):
        if data_source == "ìºì‹œëœ ë°ì´í„°":
            cached_data = load_cached_data()
            if cached_data:
                current_articles = cached_data
            else:
                st.warning("ìºì‹œëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ìƒ˜í”Œ ë°ì´í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                current_articles = get_sample_data()
        else:  # ìƒ˜í”Œ ë°ì´í„° ë˜ëŠ” ì‹¤ì‹œê°„ ìˆ˜ì§‘
            current_articles = get_sample_data()
    
    # ë°ì´í„° í•„í„°ë§
    filtered_articles = [
        article for article in current_articles 
        if (article['country'] in selected_countries and 
            article['category'] in selected_categories)
    ]
    
    if not filtered_articles:
        st.error("ì„ íƒí•œ ì¡°ê±´ì— ë§ëŠ” ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. í•„í„° ì¡°ê±´ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
        return
    
    # ë¶„ì„ê¸° ì´ˆê¸°í™”
    analyzer = TechNewsAnalyzer(filtered_articles)
    visualizer = AdvancedVisualizer(analyzer)
    
    # ìƒë‹¨ ë©”íŠ¸ë¦­ìŠ¤
    col1, col2, col3, col4, col5 = st.columns(5)
    
    with col1:
        st.markdown(
            f'''<div class="metric-card">
                <h4>ğŸ“° ì´ ê¸°ì‚¬ ìˆ˜</h4>
                <h2 style="color: #667eea;">{len(filtered_articles):,}</h2>
                <small>ì„ íƒëœ ì¡°ê±´ì˜ ê¸°ì‚¬</small>
            </div>''',
            unsafe_allow_html=True
        )
    
    with col2:
        unique_countries = len(set(article['country'] for article in filtered_articles))
        st.markdown(
            f'''<div class="metric-card">
                <h4>ğŸŒ ë¶„ì„ êµ­ê°€</h4>
                <h2 style="color: #4ECDC4;">{unique_countries}</h2>
                <small>ê°œ êµ­ê°€</small>
            </div>''',
            unsafe_allow_html=True
        )
    
    with col3:
        unique_categories = len(set(article['category'] for article in filtered_articles))
        st.markdown(
            f'''<div class="metric-card">
                <h4>ğŸ”¬ ê¸°ìˆ  ë¶„ì•¼</h4>
                <h2 style="color: #45B7D1;">{unique_categories}</h2>
                <small>ê°œ ë¶„ì•¼</small>
            </div>''',
            unsafe_allow_html=True
        )
    
    with col4:
        avg_sentiment = np.mean([article['sentiment'] for article in filtered_articles])
        sentiment_emoji = "ğŸ˜Š" if avg_sentiment > 0.1 else "ğŸ˜" if avg_sentiment >= -0.1 else "ğŸ˜Ÿ"
        sentiment_color = "#4CAF50" if avg_sentiment > 0.1 else "#FFC107" if avg_sentiment >= -0.1 else "#F44336"
        st.markdown(
            f'''<div class="metric-card">
                <h4>ğŸ­ ì „ì²´ ê°ì •</h4>
                <h2 style="color: {sentiment_color};">{sentiment_emoji} {avg_sentiment:.3f}</h2>
                <small>ê°ì • ì ìˆ˜</small>
            </div>''',
            unsafe_allow_html=True
        )
    
    with col5:
        # ìµœì‹  ê¸°ì‚¬ ë‚ ì§œ
        latest_date = max(article['date'] for article in filtered_articles)
        st.markdown(
            f'''<div class="metric-card">
                <h4>ğŸ“… ìµœì‹  ì—…ë°ì´íŠ¸</h4>
                <h2 style="color: #96CEB4; font-size: 1.2rem;">{latest_date}</h2>
                <small>ë§ˆì§€ë§‰ ê¸°ì‚¬</small>
            </div>''',
            unsafe_allow_html=True
        )
    
    st.markdown("---")
    
    # íƒ­ ë©”ë‰´
    tab1, tab2, tab3, tab4, tab5 = st.tabs([
        "ğŸ“Š ê°œìš” ëŒ€ì‹œë³´ë“œ", 
        "ğŸŒ êµ­ê°€ë³„ ë¶„ì„", 
        "ğŸ”¬ ê¸°ìˆ  íŠ¸ë Œë“œ", 
        "ğŸ•¸ï¸ ì—°ê´€ì„± ë¶„ì„", 
        "ğŸ“ˆ ì‹¬í™” ë¶„ì„"
    ])
    
    with tab1:
        st.markdown('<h3 class="sub-header">ğŸ“Š ì „ì²´ í˜„í™© ëŒ€ì‹œë³´ë“œ</h3>', unsafe_allow_html=True)
        
        col1, col2 = st.columns(2)
        
        with col1:
            # êµ­ê°€ë³„ ê¸°ì‚¬ ìˆ˜ ë¶„í¬
            country_counts = Counter([article['country'] for article in filtered_articles])
            
            fig_country = px.bar(
                x=list(country_counts.keys()),
                y=list(country_counts.values()),
                title="êµ­ê°€ë³„ ê¸°ìˆ  ì´ìŠˆ ê¸°ì‚¬ ìˆ˜",
                color=list(country_counts.values()),
                color_continuous_scale="viridis",
                text=list(country_counts.values())
            )
            fig_country.update_traces(texttemplate='%{text}', textposition='outside')
            fig_country.update_layout(showlegend=False, height=400)
            st.plotly_chart(fig_country, use_container_width=True)
        
        with col2:
            # ê¸°ìˆ  ì¹´í…Œê³ ë¦¬ë³„ ë¶„í¬
            category_counts = Counter([article['category'] for article in filtered_articles])
            
            colors = [TECH_CATEGORIES.get(cat, {}).get('color', '#888888') for cat in category_counts.keys()]
            
            fig_category = px.pie(
                values=list(category_counts.values()),
                names=list(category_counts.keys()),
                title="ê¸°ìˆ  ë¶„ì•¼ë³„ ë¹„ì¤‘",
                color_discrete_sequence=colors
            )
            fig_category.update_layout(height=400)
            st.plotly_chart(fig_category, use_container_width=True)
        
        # ì‹œê°„ë³„ íŠ¸ë Œë“œ ë¶„ì„
        st.markdown('<h4 class="sub-header">ğŸ“ˆ ì‹œê°„ë³„ ì´ìŠˆ íŠ¸ë Œë“œ</h4>', unsafe_allow_html=True)
        
        # ë‚ ì§œë³„ ë°ì´í„° ì¤€ë¹„
        df_articles = pd.DataFrame(filtered_articles)
        df_articles['date'] = pd.to_datetime(df_articles['date'])
        
        daily_counts = df_articles.groupby(['date', 'category']).size().reset_index(name='count')
        
        fig_timeline = px.line(
            daily_counts, 
            x='date', 
            y='count', 
            color='category',
            title="ì¼ë³„ ê¸°ìˆ  ì´ìŠˆ ë°œìƒ íŠ¸ë Œë“œ",
            color_discrete_map={cat: info['color'] for cat, info in TECH_CATEGORIES.items()}
        )
        fig_timeline.update_layout(height=500)
        st.plotly_chart(fig_timeline, use_container_width=True)
        
        # ì¸ì‚¬ì´íŠ¸ ë°•ìŠ¤
        insights = analyzer.generate_insights_report()
        st.markdown(
            f'''<div class="insight-box">
                <h4>ğŸ§  ì£¼ìš” ì¸ì‚¬ì´íŠ¸</h4>
                <pre style="white-space: pre-wrap; font-family: inherit; font-size: 0.9rem;">{insights}</pre>
            </div>''',
            unsafe_allow_html=True
        )
    
    with tab2:
        st.markdown('<h3 class="sub-header">ğŸŒ êµ­ê°€ë³„ ê¸°ìˆ  ì´ìŠˆ ë¶„ì„</h3>', unsafe_allow_html=True)
        
        # êµ­ê°€-ê¸°ìˆ  ë§¤íŠ¸ë¦­ìŠ¤
        matrix_data = []
        for country in selected_countries:
            country_articles = [a for a in filtered_articles if a['country'] == country]
            for category in selected_categories:
                category_count = sum(1 for a in country_articles if a['category'] == category)
                matrix_data.append({
                    'country': f"{COUNTRIES.get(country, '')} {country}",
                    'category': category,
                    'count': category_count
                })
        
        if matrix_data:
            matrix_df = pd.DataFrame(matrix_data)
            pivot_matrix = matrix_df.pivot(index='country', columns='category', values='count').fillna(0)
            
            fig_heatmap = px.imshow(
                pivot_matrix.values,
                labels=dict(x="ê¸°ìˆ  ë¶„ì•¼", y="êµ­ê°€", color="ê¸°ì‚¬ ìˆ˜"),
                x=pivot_matrix.columns,
                y=pivot_matrix.index,
                color_continuous_scale="Blues",
                title="êµ­ê°€-ê¸°ìˆ ë¶„ì•¼ íˆíŠ¸ë§µ",
                text_auto=True
            )
            fig_heatmap.update_layout(height=500)
            st.plotly_chart(fig_heatmap, use_container_width=True)
        
        # êµ­ê°€ë³„ ê°ì • ë¶„ì„
        col1, col2 = st.columns(2)
        
        with col1:
            country_sentiment = df_articles.groupby('country')['sentiment'].mean().sort_values(ascending=False)
            
            fig_sentiment = px.bar(
                x=country_sentiment.index,
                y=country_sentiment.values,
                title="êµ­ê°€ë³„ í‰ê·  ê°ì • ì ìˆ˜",
                color=country_sentiment.values,
                color_continuous_scale="RdYlGn",
                color_continuous_midpoint=0
            )
            fig_sentiment.add_hline(y=0, line_dash="dash", line_color="black", annotation_text="ì¤‘ë¦½ì„ ")
            fig_sentiment.update_layout(height=400)
            st.plotly_chart(fig_sentiment, use_container_width=True)
        
        with col2:
            # êµ­ê°€ë³„ ê¸°ì‚¬ ìˆ˜ì™€ ê°ì •ì˜ ê´€ê³„
            country_stats = df_articles.groupby('country').agg({
                'sentiment': 'mean',
                'title': 'count'
            }).rename(columns={'title': 'article_count'}).reset_index()
            
            fig_scatter = px.scatter(
                country_stats,
                x='article_count',
                y='sentiment',
                size='article_count',
                color='sentiment',
                hover_name='country',
                title="ê¸°ì‚¬ ìˆ˜ vs ê°ì • ì ìˆ˜",
                color_continuous_scale="RdYlGn",
                color_continuous_midpoint=0
            )
            fig_scatter.add_hline(y=0, line_dash="dash", line_color="black")
            fig_scatter.update_layout(height=400)
            st.plotly_chart(fig_scatter, use_container_width=True)
    
    with tab3:
        st.markdown('<h3 class="sub-header">ğŸ”¬ ê¸°ìˆ  íŠ¸ë Œë“œ ë¶„ì„</h3>', unsafe_allow_html=True)
        
        # ì‹ í¥ íŠ¸ë Œë“œ í‚¤ì›Œë“œ
        col1, col2 = st.columns([2, 1])
        
        with col1:
            trending_keywords = analyzer.detect_emerging_trends()
            
            if trending_keywords:
                trend_df = pd.DataFrame([
                    {
                        'keyword': keyword,
                        'trend_score': data['trend_score'],
                        'recent_freq': data['recent_frequency'],
                        'older_freq': data['older_frequency']
                    }
                    for keyword, data in list(trending_keywords.items())[:15]
                ])
                
                fig_trend = px.bar(
                    trend_df,
                    x='trend_score',
                    y='keyword',
                    orientation='h',
                    title="ì‹ í¥ íŠ¸ë Œë“œ í‚¤ì›Œë“œ (ìµœê·¼ 7ì¼ vs ì´ì „ ê¸°ê°„)",
                    color='trend_score',
                    color_continuous_scale="Reds",
                    hover_data=['recent_freq', 'older_freq']
                )
                fig_trend.update_layout(height=500)
                st.plotly_chart(fig_trend, use_container_width=True)
            else:
                st.info("ì¶©ë¶„í•œ ì‹œê³„ì—´ ë°ì´í„°ê°€ ì—†ì–´ íŠ¸ë Œë“œ ë¶„ì„ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        with col2:
            st.markdown("### ğŸ“Š íŠ¸ë Œë“œ í•´ì„")
            st.markdown("""
            **íŠ¸ë Œë“œ ì ìˆ˜ í•´ì„:**
            - 2.0x ì´ìƒ: ê¸‰ìƒìŠ¹ íŠ¸ë Œë“œ ğŸ”¥
            - 1.5x - 2.0x: ìƒìŠ¹ íŠ¸ë Œë“œ ğŸ“ˆ
            - 1.0x - 1.5x: ì™„ë§Œí•œ ì¦ê°€ â†—ï¸
            
            **ì£¼ì˜ì‚¬í•­:**
            - ìµœì†Œ 3íšŒ ì´ìƒ ì–¸ê¸‰ëœ í‚¤ì›Œë“œë§Œ í¬í•¨
            - ìµœê·¼ 7ì¼ vs ì´ì „ ê¸°ê°„ ë¹„êµ
            """)
            
            if trending_keywords:
                st.markdown("### ğŸ”¥ í•« í‚¤ì›Œë“œ")
                for keyword, data in list(trending_keywords.items())[:5]:
                    st.markdown(f"**{keyword}**")
                    st.markdown(f"ğŸ“ˆ {data['trend_score']:.1f}x ì¦ê°€")
                    st.markdown("---")
        
        # ê°ì • íŠ¸ë Œë“œ ë¶„ì„
        st.markdown('<h4 class="sub-header">ğŸ­ ê¸°ìˆ  ë¶„ì•¼ë³„ ê°ì • íŠ¸ë Œë“œ</h4>', unsafe_allow_html=True)
        
        sentiment_fig = visualizer.create_sentiment_timeline()
        st.plotly_chart(sentiment_fig, use_container_width=True)
    
    with tab4:
        st.markdown('<h3 class="sub-header">ğŸ•¸ï¸ ê¸°ìˆ  ê°„ ì—°ê´€ì„± ë„¤íŠ¸ì›Œí¬</h3>', unsafe_allow_html=True)
        
        # ë„¤íŠ¸ì›Œí¬ ì‹œê°í™”
        network_fig = visualizer.create_network_visualization()
        st.plotly_chart(network_fig, use_container_width=True)
        
        # í† í”½ ëª¨ë¸ë§ ê²°ê³¼
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("### ğŸ¯ ì£¼ìš” í† í”½ ë¶„ì„")
            
            try:
                topics_result = analyzer.get_trending_topics()
                
                for topic_name, topic_info in topics_result['topics'].items():
                    with st.expander(f"ğŸ“Œ {topic_name}"):
                        words = topic_info['words'][:8]
                        weights = topic_info['weight'][:8] if isinstance(topic_info['weight'], list) else [1.0] * len(words)
                        
                        topic_df = pd.DataFrame({
                            'word': words,
                            'weight': weights
                        })
                        
                        fig_topic = px.bar(
                            topic_df,
                            x='weight',
                            y='word',
                            orientation='h',
                            title=f"{topic_name} ì£¼ìš” í‚¤ì›Œë“œ"
                        )
                        fig_topic.update_layout(height=300)
                        st.plotly_chart(fig_topic, use_container_width=True)
            
            except Exception as e:
                st.warning(f"í† í”½ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        
        with col2:
            st.markdown("### ğŸ”— ê¸°ìˆ  ì—°ê´€ì„± ì¸ì‚¬ì´íŠ¸")
            
            # êµ­ê°€ë³„ ê¸°ìˆ  íŠ¹í™”ë„
            country_focus = analyzer.analyze_country_tech_focus()
            
            for country, tech_dist in list(country_focus.items())[:5]:
                if tech_dist:
                    top_tech = max(tech_dist, key=tech_dist.get)
                    focus_rate = tech_dist[top_tech]
                    flag = COUNTRIES.get(country, "ğŸ³ï¸")
                    
                    st.markdown(f"**{flag} {country}**")
                    st.markdown(f"íŠ¹í™” ë¶„ì•¼: {top_tech}")
                    st.markdown(f"ì§‘ì¤‘ë„: {focus_rate:.1%}")
                    st.markdown("---")
    
    with tab5:
        st.markdown('<h3 class="sub-header">ğŸ“ˆ ì‹¬í™” ë¶„ì„ ë° ì˜ˆì¸¡</h3>', unsafe_allow_html=True)
        
        # ì¶”ê°€ ë¶„ì„ ì˜µì…˜
        analysis_type = st.selectbox(
            "ë¶„ì„ ìœ í˜• ì„ íƒ",
            ["ê°ì • ë³€í™” íŒ¨í„´", "êµ­ê°€ ê°„ ê¸°ìˆ  ê²½ìŸë„", "ì´ìŠˆ ìƒëª…ì£¼ê¸° ë¶„ì„", "í‚¤ì›Œë“œ ë™ì‹œì¶œí˜„ ë¶„ì„"]
        )
        
        if analysis_type == "ê°ì • ë³€í™” íŒ¨í„´":
            st.markdown("### ğŸ“Š ê¸°ìˆ  ë¶„ì•¼ë³„ ê°ì • ë³€í™” íŒ¨í„´")
            
            # ê¸°ê°„ë³„ ê°ì • ë³€í™”
            df_articles['week'] = df_articles['date'].dt.isocalendar().week
            weekly_sentiment = df_articles.groupby(['week', 'category'])['sentiment'].mean().reset_index()
            
            fig_sentiment_trend = px.line(
                weekly_sentiment,
                x='week',
                y='sentiment',
                color='category',
                title="ì£¼ë³„ ê¸°ìˆ  ë¶„ì•¼ ê°ì • ë³€í™”",
                markers=True
            )
            fig_sentiment_trend.add_hline(y=0, line_dash="dash", line_color="gray")
            st.plotly_chart(fig_sentiment_trend, use_container_width=True)
        
        elif analysis_type == "êµ­ê°€ ê°„ ê¸°ìˆ  ê²½ìŸë„":
            st.markdown("### ğŸ† êµ­ê°€ë³„ ê¸°ìˆ  ê²½ìŸë ¥ ë§¤íŠ¸ë¦­ìŠ¤")
            
            # êµ­ê°€ë³„ ê¸°ìˆ  ë¶„ì•¼ ì ìœ ìœ¨
            competition_matrix = []
            for category in selected_categories:
                category_articles = [a for a in filtered_articles if a['category'] == category]
                total_articles = len(category_articles)
                
                for country in selected_countries:
                    country_articles = [a for a in category_articles if a['country'] == country]
                    share = len(country_articles) / total_articles if total_articles > 0 else 0
                    
                    competition_matrix.append({
                        'country': country,
                        'category': category,
                        'share': share,
                        'article_count': len(country_articles)
                    })
            
            comp_df = pd.DataFrame(competition_matrix)
            
            if not comp_df.empty:
                pivot_comp = comp_df.pivot(index='country', columns='category', values='share').fillna(0)
                
                fig_competition = px.imshow(
                    pivot_comp.values * 100,  # í¼ì„¼íŠ¸ë¡œ ë³€í™˜
                    labels=dict(x="ê¸°ìˆ  ë¶„ì•¼", y="êµ­ê°€", color="ì ìœ ìœ¨(%)"),
                    x=pivot_comp.columns,
                    y=pivot_comp.index,
                    color_continuous_scale="RdYlBu_r",
                    title="êµ­ê°€ë³„ ê¸°ìˆ  ë¶„ì•¼ ì ìœ ìœ¨ ë§¤íŠ¸ë¦­ìŠ¤",
                    text_auto=".1f"
                )
                fig_competition.update_layout(height=500)
                st.plotly_chart(fig_competition, use_container_width=True)
        
        elif analysis_type == "ì´ìŠˆ ìƒëª…ì£¼ê¸° ë¶„ì„":
            st.markdown("### â³ ê¸°ìˆ  ì´ìŠˆ ìƒëª…ì£¼ê¸° ë¶„ì„")
            
            # ì´ìŠˆì˜ ì§€ì† ê¸°ê°„ê³¼ ê°•ë„ ë¶„ì„
            lifecycle_data = []
            for category in selected_categories:
                cat_articles = [a for a in filtered_articles if a['category'] == category]
                if cat_articles:
                    dates = [datetime.strptime(a['date'], '%Y-%m-%d') for a in cat_articles]
                    duration = (max(dates) - min(dates)).days
                    intensity = len(cat_articles) / max(duration, 1)  # ì¼ë‹¹ ê¸°ì‚¬ ìˆ˜
                    avg_sentiment = np.mean([a['sentiment'] for a in cat_articles])
                    
                    lifecycle_data.append({
                        'category': category,
                        'duration_days': duration,
                        'intensity': intensity,
                        'avg_sentiment': avg_sentiment,
                        'total_articles': len(cat_articles)
                    })
            
            if lifecycle_data:
                lifecycle_df = pd.DataFrame(lifecycle_data)
                
                fig_lifecycle = px.scatter(
                    lifecycle_df,
                    x='duration_days',
                    y='intensity',
                    size='total_articles',
                    color='avg_sentiment',
                    hover_name='category',
                    title="ê¸°ìˆ  ì´ìŠˆ ìƒëª…ì£¼ê¸°: ì§€ì†ê¸°ê°„ vs ê°•ë„",
                    color_continuous_scale="RdYlGn",
                    color_continuous_midpoint=0
                )
                fig_lifecycle.update_layout(height=500)
                st.plotly_chart(fig_lifecycle, use_container_width=True)
        
        else:  # í‚¤ì›Œë“œ ë™ì‹œì¶œí˜„ ë¶„ì„
            st.markdown("### ğŸ”— í‚¤ì›Œë“œ ë™ì‹œì¶œí˜„ ë„¤íŠ¸ì›Œí¬")
            
            # í‚¤ì›Œë“œ ì¶”ì¶œ ë° ë™ì‹œì¶œí˜„ ë¶„ì„
            from collections import defaultdict
            import itertools
            
            cooccurrence = defaultdict(int)
            
            # ê° ê¸°ì‚¬ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
            tech_keywords = [
                'AI', 'ì¸ê³µì§€ëŠ¥', 'ë¨¸ì‹ ëŸ¬ë‹', 'ë”¥ëŸ¬ë‹',
                'ì–‘ìì»´í“¨íŒ…', 'ë¸”ë¡ì²´ì¸', 'ììœ¨ì£¼í–‰', 'IoT',
                '5G', 'ë¡œë´‡', 'ë“œë¡ ', 'ìŠ¤ë§ˆíŠ¸ì‹œí‹°',
                'ì‚¬ì´ë²„ë³´ì•ˆ', 'í•´í‚¹', 'í”„ë¼ì´ë²„ì‹œ'
            ]
            
            for article in filtered_articles:
                text = article['title'] + ' ' + article['content']
                found_keywords = [kw for kw in tech_keywords if kw.lower() in text.lower()]
                
                # í‚¤ì›Œë“œ ìŒì˜ ë™ì‹œì¶œí˜„ ì¹´ìš´íŠ¸
                for pair in itertools.combinations(found_keywords, 2):
                    sorted_pair = tuple(sorted(pair))
                    cooccurrence[sorted_pair] += 1
            
            # ìƒìœ„ ë™ì‹œì¶œí˜„ ìŒ ì‹œê°í™”
            if cooccurrence:
                cooc_data = [
                    {'keyword1': pair[0], 'keyword2': pair[1], 'count': count}
                    for pair, count in sorted(cooccurrence.items(), key=lambda x: x[1], reverse=True)[:20]
                ]
                
                cooc_df = pd.DataFrame(cooc_data)
                cooc_df['pair'] = cooc_df['keyword1'] + ' â†” ' + cooc_df['keyword2']
                
                fig_cooc = px.bar(
                    cooc_df,
                    x='count',
                    y='pair',
                    orientation='h',
                    title="í‚¤ì›Œë“œ ë™ì‹œì¶œí˜„ ë¹ˆë„ (Top 20)",
                    color='count',
                    color_continuous_scale="Viridis"
                )
                fig_cooc.update_layout(height=600)
                st.plotly_chart(fig_cooc, use_container_width=True)
            else:
                st.info("ì¶©ë¶„í•œ í‚¤ì›Œë“œ ë™ì‹œì¶œí˜„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
    
    # ìµœì‹  ê¸°ì‚¬ í”¼ë“œ
    st.markdown("---")
    st.markdown('<h3 class="sub-header">ğŸ“° ìµœì‹  ê¸°ìˆ  ì´ìŠˆ í”¼ë“œ</h3>', unsafe_allow_html=True)
    
    # ìµœì‹  ê¸°ì‚¬ 5ê°œ í‘œì‹œ
    latest_articles = sorted(filtered_articles, key=lambda x: x['date'], reverse=True)[:10]
    
    for i, article in enumerate(latest_articles):
        with st.expander(f"ğŸ“„ {article['title']}", expanded=(i < 3)):  # ì²˜ìŒ 3ê°œëŠ” í¼ì³ì§„ ìƒíƒœ
            col1, col2, col3 = st.columns([3, 1, 1])
            
            with col1:
                st.write(article['content'])
                
                # ê¸°ì‚¬ URL ë§í¬
                if article.get('url'):
                    st.markdown(f"ğŸ”— [ì›ë¬¸ ë³´ê¸°]({article['url']})")
            
            with col2:
                # ì¹´í…Œê³ ë¦¬ ë°°ì§€
                category_colors = TECH_CATEGORIES
                color = category_colors.get(article['category'], {}).get('color', '#888888')
                st.markdown(
                    f'<span class="tech-category" style="background-color: {color}">{article["category"]}</span>',
                    unsafe_allow_html=True
                )
                
                flag = COUNTRIES.get(article['country'], "ğŸ³ï¸")
                st.write(f"**ğŸŒ êµ­ê°€:** {flag} {article['country']}")
                st.write(f"**ğŸ“… ë‚ ì§œ:** {article['date']}")
                st.write(f"**ğŸ“° ì¶œì²˜:** {article['source']}")
            
            with col3:
                # ê°ì • ë¶„ì„ ê²°ê³¼
                sentiment = article['sentiment']
                if sentiment > 0.1:
                    sentiment_emoji = "ğŸ˜Š"
                    sentiment_color = "#4CAF50"
                    sentiment_text = "ê¸ì •ì "
                elif sentiment < -0.1:
                    sentiment_emoji = "ğŸ˜Ÿ"
                    sentiment_color = "#F44336"
                    sentiment_text = "ë¶€ì •ì "
                else:
                    sentiment_emoji = "ğŸ˜"
                    sentiment_color = "#FFC107"
                    sentiment_text = "ì¤‘ë¦½ì "
                
                st.markdown(f"**ğŸ­ ê°ì • ë¶„ì„**")
                st.markdown(
                    f'<div style="text-align: center; color: {sentiment_color};">'
                    f'<div style="font-size: 2rem;">{sentiment_emoji}</div>'
                    f'<div>{sentiment_text}</div>'
                    f'<div style="font-size: 0.9rem;">({sentiment:.3f})</div>'
                    f'</div>',
                    unsafe_allow_html=True
                )
    
    # í‘¸í„°
    st.markdown("---")
    st.markdown(
        f'''<div class="data-source">
            <p>ğŸ“Š <strong>ë°ì´í„° ì†ŒìŠ¤:</strong> {data_source} | 
            ğŸ”„ <strong>ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸:</strong> {datetime.now().strftime("%Y-%m-%d %H:%M:%S")} | 
            ğŸ“ˆ <strong>ë¶„ì„ ê¸°ì‚¬ ìˆ˜:</strong> {len(filtered_articles):,}ê°œ</p>
        </div>''',
        unsafe_allow_html=True
    )
    
    st.markdown(
        """
        <div style="text-align: center; color: #666; padding: 2rem; background: #f8f9fa; border-radius: 10px; margin-top: 2rem;">
            <h4>ğŸ”¬ êµ­ì œ ì‹œì‚¬ íƒêµ¬ ë™ì•„ë¦¬ - ê¸€ë¡œë²Œ ê¸°ìˆ  ì´ìŠˆ ë¶„ì„ í”„ë¡œì íŠ¸</h4>
            <p>ë°ì´í„° ê¸°ë°˜ ì˜ì‚¬ê²°ì •ê³¼ ë¯¸ë˜ ê¸°ìˆ  íŠ¸ë Œë“œ ì˜ˆì¸¡ì„ í†µí•œ í†µì°°ë ¥ ê°œë°œ</p>
            <p><small>Made with â¤ï¸ using Streamlit & Python | ğŸ‰ ì„±ê³µì ìœ¼ë¡œ ë°°í¬ë˜ì—ˆìŠµë‹ˆë‹¤!</small></p>
        </div>
        """,
        unsafe_allow_html=True
    )

if __name__ == "__main__":
    main()
